{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4eec075-da51-407e-9a17-e8740f3acde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "INPUT_FILE = \"/Users/tenzinchoedhen/Desktop/All Recorded Traffic.csv\"\n",
    "OUTPUT_FILE = \"/Users/tenzinchoedhen/Desktop/weather_data.csv\"\n",
    "\n",
    "DATE_COL = \"DATE\"\n",
    "TIME_COL = \"TIME\"\n",
    "CHUNKSIZE = 300_000\n",
    "\n",
    "\n",
    "def hhmm_to_hh_colon_mm(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Convert HHMM integers/strings (e.g., 0, 15, 945, 1300, 2359) -> 'HH:MM'\n",
    "    \"\"\"\n",
    "    s = s.astype(\"Int64\").astype(str)          # keeps NA safe\n",
    "    s = s.str.replace(r\"\\.0$\", \"\", regex=True) # in case it was read as float\n",
    "    s = s.str.zfill(4)                         # 945 -> '0945'\n",
    "    return s.str[:2] + \":\" + s.str[2:4]\n",
    "\n",
    "\n",
    "def build_weather_dataset_big(input_file: str, output_file: str) -> None:\n",
    "    seen = set()\n",
    "    out_chunks = []\n",
    "\n",
    "    for chunk in pd.read_csv(\n",
    "        input_file,\n",
    "        usecols=[DATE_COL, TIME_COL],\n",
    "        chunksize=CHUNKSIZE\n",
    "    ):\n",
    "        # --- Date to mm/dd/yyyy ---\n",
    "        chunk[DATE_COL] = pd.to_datetime(chunk[DATE_COL], errors=\"coerce\")\n",
    "        chunk = chunk.dropna(subset=[DATE_COL])\n",
    "        chunk[DATE_COL] = chunk[DATE_COL].dt.strftime(\"%m/%d/%Y\")\n",
    "\n",
    "        # --- Time HHMM -> HH:MM ---\n",
    "        chunk = chunk.dropna(subset=[TIME_COL])\n",
    "        chunk[TIME_COL] = hhmm_to_hh_colon_mm(chunk[TIME_COL])\n",
    "\n",
    "        # --- Primary Key (Date + Time) ---\n",
    "        chunk[\"pk_date_time\"] = chunk[DATE_COL] + \"_\" + chunk[TIME_COL]\n",
    "\n",
    "        # --- Keeping only unique Date+Time across the full file ---\n",
    "        is_new = ~chunk[\"pk_date_time\"].isin(seen)\n",
    "        new_rows = chunk.loc[is_new, [DATE_COL, TIME_COL, \"pk_date_time\"]]\n",
    "\n",
    "        seen.update(new_rows[\"pk_date_time\"].tolist())\n",
    "        out_chunks.append(new_rows[[DATE_COL, TIME_COL]])\n",
    "\n",
    "    result = pd.concat(out_chunks, ignore_index=True).drop_duplicates()\n",
    "    result.to_csv(output_file, index=False)\n",
    "\n",
    "    print(\"Saved:\", output_file)\n",
    "    print(\"Unique Date+Time rows:\", len(result))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    build_weather_dataset_big(INPUT_FILE, OUTPUT_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f724fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "WEATHER_INPUT = \"/Users/tenzinchoedhen/Desktop/All Recorded Traffic.csv\"\n",
    "WEATHER_OUTPUT = \"/Users/tenzinchoedhen/Desktop/All Recorded Traffic.csv\"  # overwrite (or change name)\n",
    "\n",
    "# ---- Choose ONE location (example: Manhattan / Midtown-ish) ----\n",
    "LAT = 40.7580\n",
    "LON = -73.9855\n",
    "\n",
    "# Use a real timezone for correct local-hour alignment\n",
    "TIMEZONE = \"America/New_York\"\n",
    "\n",
    "# Pick the hourly fields you want from Open-Meteo\n",
    "HOURLY_VARS = [\n",
    "    \"temperature_2m\",\n",
    "    \"relative_humidity_2m\",\n",
    "    \"precipitation\",\n",
    "    \"rain\",\n",
    "    \"snowfall\",\n",
    "    \"cloud_cover\",\n",
    "    \"wind_speed_10m\",\n",
    "    \"wind_direction_10m\",\n",
    "    \"weather_code\",\n",
    "]\n",
    "\n",
    "def fetch_open_meteo_hourly(lat, lon, start_date, end_date, timezone):\n",
    "    \"\"\"\n",
    "    Calls Open-Meteo Historical Weather API (/v1/archive) and returns a DataFrame of hourly results.\n",
    "    \"\"\"\n",
    "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "    params = {\n",
    "        \"latitude\": lat,\n",
    "        \"longitude\": lon,\n",
    "        \"start_date\": start_date,  # YYYY-MM-DD\n",
    "        \"end_date\": end_date,      # YYYY-MM-DD\n",
    "        \"hourly\": \",\".join(HOURLY_VARS),\n",
    "        \"timezone\": timezone,\n",
    "    }\n",
    "\n",
    "    r = requests.get(url, params=params, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "\n",
    "    hourly = data.get(\"hourly\", {})\n",
    "    if not hourly or \"time\" not in hourly:\n",
    "        raise ValueError(\"No hourly data returned. Check dates/lat/lon/timezone.\")\n",
    "\n",
    "    weather_df = pd.DataFrame(hourly)\n",
    "    # 'time' comes like 'YYYY-MM-DDTHH:MM'\n",
    "    weather_df[\"datetime\"] = pd.to_datetime(weather_df[\"time\"])\n",
    "    weather_df.drop(columns=[\"time\"], inplace=True)\n",
    "\n",
    "    return weather_df\n",
    "\n",
    "def integrate_weather():\n",
    "    # Load your unique Date+Time dataset\n",
    "    df = pd.read_csv(WEATHER_INPUT, dtype=str)\n",
    "\n",
    "    # Build a datetime column from Date (mm/dd/yyyy) + Time (HH:MM)\n",
    "    df[\"datetime\"] = pd.to_datetime(df[\"DATE\"] + \" \" + df[\"TIME\"], format=\"%m/%d/%Y %H:%M\", errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"datetime\"])\n",
    "\n",
    "    # Determine weather date range (Open-Meteo wants YYYY-MM-DD)\n",
    "    start_date = df[\"datetime\"].min().date().isoformat()\n",
    "    end_date = df[\"datetime\"].max().date().isoformat()\n",
    "\n",
    "    # Fetch weather for full range (hourly)\n",
    "    weather_df = fetch_open_meteo_hourly(LAT, LON, start_date, end_date, TIMEZONE)\n",
    "\n",
    "    # Merge on datetime (left join keeps all your Date+Time rows)\n",
    "    merged = df.merge(weather_df, on=\"datetime\", how=\"left\")\n",
    "\n",
    "    # Optional: drop datetime column if you donâ€™t want it in final CSV\n",
    "    # merged = merged.drop(columns=[\"datetime\"])\n",
    "\n",
    "    merged.to_csv(WEATHER_OUTPUT, index=False)\n",
    "    print(\"Saved with weather:\", WEATHER_OUTPUT)\n",
    "    print(\"Rows:\", len(merged))\n",
    "    print(\"Weather columns added:\", [c for c in merged.columns if c not in [\"Date\", \"Time\", \"datetime\"]])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    integrate_weather()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3b4a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIG\n",
    "# -----------------------------\n",
    "INPUT_FILE  = \"/Users/tenzinchoedhen/Desktop/weather_data.csv\"   # your current file (already has weather_code)\n",
    "OUTPUT_FILE = \"/Users/tenzinchoedhen/Desktop/weather_data.csv\"   # overwrite; change if you want a new file\n",
    "\n",
    "# -----------------------------\n",
    "# WEATHER CODE -> DESCRIPTION\n",
    "# -----------------------------\n",
    "def weather_code_to_description(code):\n",
    "    wmo_map = {\n",
    "        0: \"Clear sky\",\n",
    "        1: \"Mainly clear\",\n",
    "        2: \"Partly cloudy\",\n",
    "        3: \"Overcast\",\n",
    "        45: \"Fog\",\n",
    "        48: \"Depositing rime fog\",\n",
    "        51: \"Light drizzle\",\n",
    "        53: \"Moderate drizzle\",\n",
    "        55: \"Dense drizzle\",\n",
    "        56: \"Light freezing drizzle\",\n",
    "        57: \"Dense freezing drizzle\",\n",
    "        61: \"Slight rain\",\n",
    "        63: \"Moderate rain\",\n",
    "        65: \"Heavy rain\",\n",
    "        66: \"Light freezing rain\",\n",
    "        67: \"Heavy freezing rain\",\n",
    "        71: \"Slight snowfall\",\n",
    "        73: \"Moderate snowfall\",\n",
    "        75: \"Heavy snowfall\",\n",
    "        77: \"Snow grains\",\n",
    "        80: \"Slight rain showers\",\n",
    "        81: \"Moderate rain showers\",\n",
    "        82: \"Violent rain showers\",\n",
    "        85: \"Slight snow showers\",\n",
    "        86: \"Heavy snow showers\",\n",
    "        95: \"Thunderstorm\",\n",
    "        96: \"Thunderstorm with slight hail\",\n",
    "        99: \"Thunderstorm with heavy hail\",\n",
    "    }\n",
    "    return wmo_map.get(code, \"Unknown\")\n",
    "\n",
    "def add_weather_description(input_file: str, output_file: str) -> None:\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    if \"weather_code\" not in df.columns:\n",
    "        raise ValueError(f\"'weather_code' column not found. Columns are: {list(df.columns)}\")\n",
    "\n",
    "    df[\"weather_code\"] = pd.to_numeric(df[\"weather_code\"], errors=\"coerce\")\n",
    "\n",
    "    df[\"weather_description\"] = df[\"weather_code\"].apply(\n",
    "        lambda x: weather_code_to_description(int(x)) if pd.notna(x) else \"Unknown\"\n",
    "    )\n",
    "\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(\"Saved:\", output_file)\n",
    "    print(\"Rows:\", len(df))\n",
    "    print(\"Missing weather_code:\", df[\"weather_code\"].isna().sum())\n",
    "    print(\"Sample:\")\n",
    "    print(df[[\"weather_code\", \"weather_description\"]].dropna().head(10))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    add_weather_description(INPUT_FILE, OUTPUT_FILE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7792ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import holidays\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIG\n",
    "# -----------------------------\n",
    "INPUT_FILE  = \"/Users/tenzinchoedhen/Desktop/weather_data.csv\"\n",
    "OUTPUT_FILE = \"/Users/tenzinchoedhen/Desktop/weather_data.csv\"  # overwrite if you want\n",
    "\n",
    "DATE_COL = \"DATE\"   # change to \"Date\" if your file uses that\n",
    "TIME_COL = \"TIME\"   # change to \"Time\" if your file uses that\n",
    "\n",
    "# NYC Open Data: NYC Permitted Event Information\n",
    "NYC_EVENTS_DATASET = \"tvpp-9vvx\"\n",
    "NYC_APP_TOKEN = None  # optional\n",
    "NYC_LIMIT = 50000\n",
    "\n",
    "# -----------------------------\n",
    "# HELPERS\n",
    "# -----------------------------\n",
    "def build_datetime(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"datetime\"] = pd.to_datetime(\n",
    "        df[DATE_COL].astype(str).str.strip() + \" \" + df[TIME_COL].astype(str).str.strip(),\n",
    "        format=\"%m/%d/%Y %H:%M\",\n",
    "        errors=\"coerce\",\n",
    "    )\n",
    "    return df.dropna(subset=[\"datetime\"])\n",
    "\n",
    "\n",
    "def add_holiday_flags(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    years = sorted(df[\"datetime\"].dt.year.unique().tolist())\n",
    "    ny = holidays.US(state=\"NY\", years=years)\n",
    "    nj = holidays.US(state=\"NJ\", years=years)\n",
    "\n",
    "    d = df[\"datetime\"].dt.date\n",
    "    df[\"holiday_ny_name\"] = [ny.get(x) for x in d]\n",
    "    df[\"holiday_nj_name\"] = [nj.get(x) for x in d]\n",
    "\n",
    "    df[\"is_holiday_ny\"] = df[\"holiday_ny_name\"].notna()\n",
    "    df[\"is_holiday_nj\"] = df[\"holiday_nj_name\"].notna()\n",
    "    df[\"is_holiday_any\"] = df[\"is_holiday_ny\"] | df[\"is_holiday_nj\"]\n",
    "    df[\"is_weekend\"] = df[\"datetime\"].dt.dayofweek >= 5\n",
    "    return df\n",
    "\n",
    "\n",
    "def soda_get(url: str, params: dict) -> list:\n",
    "    headers = {}\n",
    "    if NYC_APP_TOKEN:\n",
    "        headers[\"X-App-Token\"] = NYC_APP_TOKEN\n",
    "    r = requests.get(url, params=params, headers=headers, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "\n",
    "def fetch_nyc_events(start_dt: pd.Timestamp, end_dt: pd.Timestamp) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Pull NYC permitted events and keep only events overlapping your datetime window.\n",
    "    Dataset reference: tvpp-9vvx.\n",
    "    \"\"\"\n",
    "    url = f\"https://data.cityofnewyork.us/resource/{NYC_EVENTS_DATASET}.json\"\n",
    "    params = {\"$limit\": NYC_LIMIT}\n",
    "\n",
    "    rows = soda_get(url, params)\n",
    "    ev = pd.DataFrame(rows)\n",
    "    if ev.empty:\n",
    "        return ev\n",
    "\n",
    "    # fields in this dataset commonly include start_date_time / end_date_time\n",
    "    start_field = \"start_date_time\"\n",
    "    end_field = \"end_date_time\"\n",
    "\n",
    "    if start_field not in ev.columns:\n",
    "        raise ValueError(f\"NYC events start field not found. Columns: {list(ev.columns)}\")\n",
    "\n",
    "    ev[\"event_start\"] = pd.to_datetime(ev[start_field], errors=\"coerce\")\n",
    "    ev[\"event_end\"] = pd.to_datetime(ev[end_field], errors=\"coerce\") if end_field in ev.columns else ev[\"event_start\"]\n",
    "    ev = ev.dropna(subset=[\"event_start\"])\n",
    "    ev[\"event_end\"] = ev[\"event_end\"].fillna(ev[\"event_start\"])\n",
    "\n",
    "    # overlap filter\n",
    "    ev = ev[(ev[\"event_end\"] >= start_dt) & (ev[\"event_start\"] <= end_dt)].copy()\n",
    "    return ev\n",
    "\n",
    "\n",
    "def make_event_hour_index(events_df: pd.DataFrame) -> pd.DatetimeIndex:\n",
    "    \"\"\"\n",
    "    Convert event windows into a set of hourly timestamps.\n",
    "    Uses 'h' (lowercase) to avoid pandas FutureWarning.\n",
    "    \"\"\"\n",
    "    hours = set()\n",
    "    for s, e in events_df[[\"event_start\", \"event_end\"]].itertuples(index=False):\n",
    "        if pd.isna(s) or pd.isna(e):\n",
    "            continue\n",
    "        s = pd.to_datetime(s).floor(\"h\")\n",
    "        e = pd.to_datetime(e).ceil(\"h\")\n",
    "        for t in pd.date_range(s, e, freq=\"h\"):\n",
    "            hours.add(t)\n",
    "    return pd.DatetimeIndex(sorted(hours))\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# MAIN\n",
    "# -----------------------------\n",
    "def integrate_holidays_and_nyc_events():\n",
    "    df = pd.read_csv(INPUT_FILE, dtype=str)\n",
    "    df = build_datetime(df)\n",
    "\n",
    "    start_dt = df[\"datetime\"].min()\n",
    "    end_dt = df[\"datetime\"].max()\n",
    "\n",
    "    # 1) Holidays\n",
    "    df = add_holiday_flags(df)\n",
    "\n",
    "    # 2) NYC permitted events (hour-level flag)\n",
    "    nyc_events = fetch_nyc_events(start_dt, end_dt)\n",
    "    if not nyc_events.empty:\n",
    "        nyc_event_hours = make_event_hour_index(nyc_events)\n",
    "        df[\"is_nyc_permitted_event_hour\"] = df[\"datetime\"].isin(nyc_event_hours)\n",
    "    else:\n",
    "        df[\"is_nyc_permitted_event_hour\"] = False\n",
    "\n",
    "    df.to_csv(OUTPUT_FILE, index=False)\n",
    "    print(\"Saved:\", OUTPUT_FILE)\n",
    "    print(\"Rows:\", len(df))\n",
    "    print(\"Holiday(any):\", int(df[\"is_holiday_any\"].sum()))\n",
    "    print(\"NYC event hours:\", int(df[\"is_nyc_permitted_event_hour\"].sum()))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    integrate_holidays_and_nyc_events()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00c9b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIG\n",
    "# -----------------------------\n",
    "WEATHER_FILE = \"/Users/tenzinchoedhen/Desktop/weather_data.csv\"  # will be overwritten\n",
    "DATE_COL = \"DATE\"\n",
    "TIME_COL = \"TIME\"\n",
    "\n",
    "CACHE_DIR = \"/Users/tenzinchoedhen/Desktop/schedule_cache\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "LOCAL_TZ = \"America/New_York\"\n",
    "\n",
    "# -----------------------------\n",
    "# PRE + POST WINDOWS (hours)\n",
    "# -----------------------------\n",
    "# Traffic typically increases BEFORE (arrivals) and AFTER (exit).\n",
    "NFL_PRE, NFL_POST = 2, 4\n",
    "MLS_PRE, MLS_POST = 1, 3\n",
    "MLB_PRE, MLB_POST = 1, 4\n",
    "NBA_PRE, NBA_POST = 1, 3\n",
    "NHL_PRE, NHL_POST = 1, 3\n",
    "\n",
    "# -----------------------------\n",
    "# SOURCES (download once)\n",
    "# -----------------------------\n",
    "# NFL: all games CSV (filter by stadium MetLife/Giants Stadium)\n",
    "NFL_GAMES_CSV_URL = \"https://raw.githubusercontent.com/nflverse/nfldata/master/data/games.csv\"\n",
    "NFL_GAMES_CSV_PATH = os.path.join(CACHE_DIR, \"nfl_games.csv\")\n",
    "\n",
    "# MLS team calendars (ICS)\n",
    "RBNY_ICS_URL = \"https://mlscalendar.jeffsoftware.com/ical/new-york-red-bulls.ics\"\n",
    "NYCFC_ICS_URL = \"https://mlscalendar.jeffsoftware.com/ical/new-york-city-fc.ics\"\n",
    "RBNY_ICS_PATH = os.path.join(CACHE_DIR, \"mls_rbny.ics\")\n",
    "NYCFC_ICS_PATH = os.path.join(CACHE_DIR, \"mls_nycfc.ics\")\n",
    "\n",
    "# MLB \"downloadable schedule\" pages (extract CSV link)\n",
    "YANKEES_CSV_PAGE = \"https://www.mlb.com/yankees/schedule/downloadable-schedule\"\n",
    "METS_CSV_PAGE = \"https://www.mlb.com/mets/schedule/downloadable-schedule\"\n",
    "YANKEES_CSV_PATH = os.path.join(CACHE_DIR, \"mlb_yankees.csv\")\n",
    "METS_CSV_PATH = os.path.join(CACHE_DIR, \"mlb_mets.csv\")\n",
    "\n",
    "# NBA via FixtureDownload (Knicks + Nets)\n",
    "NBA_TEAMS = [\"new-york-knicks\", \"brooklyn-nets\"]\n",
    "NBA_LOCAL_VENUES = [\"Madison Square Garden\", \"Barclays Center\"]\n",
    "\n",
    "# NHL official team CSV feeds (Rangers/Islanders/Devils)\n",
    "NHL_TEAMS = [\"nyr\", \"nyi\", \"njd\"]\n",
    "NHL_FEED_TEMPLATE = \"https://www.nhl.com/feed/nhl/ics/schedule.csv?format=csv&homeaway=full&team={team}\"\n",
    "NHL_LOCAL_VENUES = [\"Madison Square Garden\", \"UBS Arena\", \"Prudential Center\"]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# CORE HELPERS\n",
    "# -----------------------------\n",
    "def build_weather_datetime(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"datetime\"] = pd.to_datetime(\n",
    "        df[DATE_COL].astype(str).str.strip() + \" \" + df[TIME_COL].astype(str).str.strip(),\n",
    "        format=\"%m/%d/%Y %H:%M\",\n",
    "        errors=\"coerce\",\n",
    "    )\n",
    "    return df.dropna(subset=[\"datetime\"])\n",
    "\n",
    "\n",
    "def window_hours(start_local_naive: pd.Timestamp, pre_hours: int, post_hours: int) -> pd.DatetimeIndex:\n",
    "    \"\"\"\n",
    "    Hourly stamps covering [start - pre_hours, start + post_hours].\n",
    "    \"\"\"\n",
    "    s = (start_local_naive - pd.Timedelta(hours=pre_hours)).floor(\"h\")\n",
    "    e = (start_local_naive + pd.Timedelta(hours=post_hours)).ceil(\"h\")\n",
    "    return pd.date_range(s, e, freq=\"h\")\n",
    "\n",
    "\n",
    "def utc_to_local_naive(ts_utc: pd.Timestamp) -> pd.Timestamp:\n",
    "    # schedule side conversion only (safe)\n",
    "    return ts_utc.tz_convert(LOCAL_TZ).tz_localize(None)\n",
    "\n",
    "\n",
    "def download_bytes(url: str) -> tuple[bytes, str]:\n",
    "    r = requests.get(url, timeout=60, allow_redirects=True)\n",
    "    r.raise_for_status()\n",
    "    return r.content, r.headers.get(\"Content-Type\", \"\").lower()\n",
    "\n",
    "\n",
    "def save_file(path: str, content: bytes) -> None:\n",
    "    with open(path, \"wb\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "\n",
    "def download_if_missing(url: str, path: str) -> None:\n",
    "    if os.path.exists(path) and os.path.getsize(path) > 0:\n",
    "        return\n",
    "    content, _ = download_bytes(url)\n",
    "    save_file(path, content)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# NFL (MetLife) from nflverse games.csv\n",
    "# -----------------------------\n",
    "def load_nfl_metlife_hours(min_dt: pd.Timestamp, max_dt: pd.Timestamp) -> set[pd.Timestamp]:\n",
    "    download_if_missing(NFL_GAMES_CSV_URL, NFL_GAMES_CSV_PATH)\n",
    "    games = pd.read_csv(NFL_GAMES_CSV_PATH)\n",
    "\n",
    "    games[\"gameday\"] = pd.to_datetime(games.get(\"gameday\"), errors=\"coerce\")\n",
    "    games[\"gametime\"] = games.get(\"gametime\", \"\").astype(str).str.strip()\n",
    "    games[\"stadium\"] = games.get(\"stadium\", \"\").astype(str)\n",
    "\n",
    "    games = games[games[\"gameday\"].notna() & (games[\"gametime\"].str.len() > 0)].copy()\n",
    "\n",
    "    games[\"game_dt_local_naive\"] = pd.to_datetime(\n",
    "        games[\"gameday\"].dt.strftime(\"%Y-%m-%d\") + \" \" + games[\"gametime\"],\n",
    "        errors=\"coerce\",\n",
    "    )\n",
    "    games = games.dropna(subset=[\"game_dt_local_naive\"])\n",
    "\n",
    "    # filter to your range (+2 days buffer because we have pre-window)\n",
    "    start = min_dt.normalize() - pd.Timedelta(days=2)\n",
    "    end = max_dt.normalize() + pd.Timedelta(days=2)\n",
    "    games = games[(games[\"game_dt_local_naive\"] >= start) & (games[\"game_dt_local_naive\"] <= end)]\n",
    "\n",
    "    metlife = games[games[\"stadium\"].str.contains(\"MetLife Stadium|Giants Stadium\", case=False, na=False)]\n",
    "\n",
    "    hours = set()\n",
    "    for dt in metlife[\"game_dt_local_naive\"]:\n",
    "        for h in window_hours(dt, NFL_PRE, NFL_POST):\n",
    "            hours.add(h)\n",
    "    return hours\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Minimal ICS parsing (MLS)\n",
    "# -----------------------------\n",
    "def parse_ics_events(text: str) -> list[dict]:\n",
    "    unfolded = re.sub(r\"\\r?\\n[ \\t]\", \"\", text)\n",
    "    out = []\n",
    "    for part in unfolded.split(\"BEGIN:VEVENT\"):\n",
    "        if \"END:VEVENT\" not in part:\n",
    "            continue\n",
    "        block = part.split(\"END:VEVENT\")[0]\n",
    "\n",
    "        def get(prefix: str):\n",
    "            m = re.search(rf\"{prefix}(?:;[^:]*)?:([^\\r\\n]+)\", block)\n",
    "            return m.group(1).strip() if m else None\n",
    "\n",
    "        dtstart = get(\"DTSTART\")\n",
    "        if dtstart:\n",
    "            out.append({\"dtstart\": dtstart})\n",
    "    return out\n",
    "\n",
    "\n",
    "def ics_dt_to_local_naive(dt_raw: str) -> pd.Timestamp | None:\n",
    "    dt_raw = dt_raw.strip()\n",
    "\n",
    "    # UTC format: 20260221T193000Z\n",
    "    if dt_raw.endswith(\"Z\"):\n",
    "        ts = pd.to_datetime(dt_raw, format=\"%Y%m%dT%H%M%SZ\", utc=True, errors=\"coerce\")\n",
    "        if pd.isna(ts):\n",
    "            return None\n",
    "        return utc_to_local_naive(ts)\n",
    "\n",
    "    # floating local: 20260221T193000 or 20260221T1930\n",
    "    ts = pd.to_datetime(dt_raw, format=\"%Y%m%dT%H%M%S\", errors=\"coerce\")\n",
    "    if pd.isna(ts):\n",
    "        ts = pd.to_datetime(dt_raw, format=\"%Y%m%dT%H%M\", errors=\"coerce\")\n",
    "    return None if pd.isna(ts) else ts\n",
    "\n",
    "\n",
    "def load_mls_hours(ics_url: str, ics_path: str, min_dt: pd.Timestamp, max_dt: pd.Timestamp, pre_h: int, post_h: int) -> set[pd.Timestamp]:\n",
    "    download_if_missing(ics_url, ics_path)\n",
    "    with open(ics_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    events = parse_ics_events(text)\n",
    "\n",
    "    start = min_dt.normalize() - pd.Timedelta(days=2)\n",
    "    end = max_dt.normalize() + pd.Timedelta(days=2)\n",
    "\n",
    "    hours = set()\n",
    "    for e in events:\n",
    "        dt = ics_dt_to_local_naive(e[\"dtstart\"])\n",
    "        if dt is None or not (start <= dt <= end):\n",
    "            continue\n",
    "        for h in window_hours(dt, pre_h, post_h):\n",
    "            hours.add(h)\n",
    "    return hours\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# MLB download page -> CSV -> parse\n",
    "# -----------------------------\n",
    "def download_mlb_csv_from_page(page_url: str, out_csv_path: str) -> bool:\n",
    "    try:\n",
    "        content, content_type = download_bytes(page_url)\n",
    "\n",
    "        # if actual CSV returned\n",
    "        head = content[:80].decode(\"utf-8\", errors=\"ignore\").lower()\n",
    "        if \"text/csv\" in content_type or head.startswith(\"date\"):\n",
    "            save_file(out_csv_path, content)\n",
    "            return True\n",
    "\n",
    "        html = content.decode(\"utf-8\", errors=\"ignore\")\n",
    "        m = re.search(r'href=\"([^\"]+\\.csv[^\"]*)\"', html, flags=re.IGNORECASE)\n",
    "        if not m:\n",
    "            return False\n",
    "\n",
    "        csv_url = m.group(1)\n",
    "        if csv_url.startswith(\"/\"):\n",
    "            csv_url = \"https://www.mlb.com\" + csv_url\n",
    "\n",
    "        csv_bytes, _ = download_bytes(csv_url)\n",
    "        save_file(out_csv_path, csv_bytes)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def load_mlb_hours(csv_path: str, min_dt: pd.Timestamp, max_dt: pd.Timestamp, pre_h: int, post_h: int) -> set[pd.Timestamp]:\n",
    "    sch = pd.read_csv(csv_path)\n",
    "\n",
    "    cols = {c.lower(): c for c in sch.columns}\n",
    "    date_col = cols.get(\"date\") or cols.get(\"game date\") or cols.get(\"gamedate\")\n",
    "    time_col = cols.get(\"time\") or cols.get(\"start time\") or cols.get(\"start_time\")\n",
    "    if not date_col or not time_col:\n",
    "        return set()\n",
    "\n",
    "    sch[\"_date\"] = pd.to_datetime(sch[date_col], errors=\"coerce\")\n",
    "    sch[\"_time\"] = sch[time_col].astype(str).str.strip()\n",
    "    sch[\"_dt\"] = pd.to_datetime(sch[\"_date\"].dt.strftime(\"%Y-%m-%d\") + \" \" + sch[\"_time\"], errors=\"coerce\")\n",
    "    sch = sch.dropna(subset=[\"_dt\"])\n",
    "\n",
    "    start = min_dt.normalize() - pd.Timedelta(days=2)\n",
    "    end = max_dt.normalize() + pd.Timedelta(days=2)\n",
    "    sch = sch[(sch[\"_dt\"] >= start) & (sch[\"_dt\"] <= end)]\n",
    "\n",
    "    hours = set()\n",
    "    for dt in sch[\"_dt\"]:\n",
    "        for h in window_hours(dt, pre_h, post_h):\n",
    "            hours.add(h)\n",
    "    return hours\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# NBA (Knicks/Nets) from FixtureDownload\n",
    "# -----------------------------\n",
    "def resolve_fixturedownload_csv(team_slug: str, season_slug: str) -> str | None:\n",
    "    url = f\"https://fixturedownload.com/download/csv/{season_slug}/{team_slug}\"\n",
    "    try:\n",
    "        html_bytes, content_type = download_bytes(url)\n",
    "\n",
    "        if \"text/csv\" in content_type:\n",
    "            out_path = os.path.join(CACHE_DIR, f\"nba_{season_slug}_{team_slug}.csv\")\n",
    "            save_file(out_path, html_bytes)\n",
    "            return out_path\n",
    "\n",
    "        html = html_bytes.decode(\"utf-8\", errors=\"ignore\")\n",
    "        m = re.search(r\"https://fixturedownload\\.com/download/[^\\s\\\"']+\\.csv\", html, flags=re.IGNORECASE)\n",
    "        if not m:\n",
    "            return None\n",
    "\n",
    "        direct_csv_url = m.group(0)\n",
    "        csv_bytes, _ = download_bytes(direct_csv_url)\n",
    "        out_path = os.path.join(CACHE_DIR, f\"nba_{season_slug}_{team_slug}.csv\")\n",
    "        save_file(out_path, csv_bytes)\n",
    "        return out_path\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def nba_season_slugs_from_range(min_dt: pd.Timestamp, max_dt: pd.Timestamp) -> list[str]:\n",
    "    return [f\"nba-{y}\" for y in range(min_dt.year - 1, max_dt.year + 1)]\n",
    "\n",
    "\n",
    "def load_nba_hours(min_dt: pd.Timestamp, max_dt: pd.Timestamp, pre_h: int, post_h: int) -> set[pd.Timestamp]:\n",
    "    start = min_dt.normalize() - pd.Timedelta(days=2)\n",
    "    end = max_dt.normalize() + pd.Timedelta(days=2)\n",
    "\n",
    "    hours = set()\n",
    "    for season_slug in nba_season_slugs_from_range(min_dt, max_dt):\n",
    "        for team_slug in NBA_TEAMS:\n",
    "            csv_path = resolve_fixturedownload_csv(team_slug, season_slug)\n",
    "            if not csv_path:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                sch = pd.read_csv(csv_path)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            if \"Date\" not in sch.columns or \"Location\" not in sch.columns:\n",
    "                continue\n",
    "\n",
    "            # only NY/NJ venues\n",
    "            local_mask = False\n",
    "            for v in NBA_LOCAL_VENUES:\n",
    "                local_mask = local_mask | sch[\"Location\"].astype(str).str.contains(v, case=False, na=False)\n",
    "            sch = sch[local_mask].copy()\n",
    "\n",
    "            # FixtureDownload commonly uses UTC in \"DD/MM/YYYY HH:MM\"\n",
    "            sch[\"start_utc\"] = pd.to_datetime(sch[\"Date\"], format=\"%d/%m/%Y %H:%M\", errors=\"coerce\", utc=True)\n",
    "            sch = sch.dropna(subset=[\"start_utc\"])\n",
    "            sch[\"start_local_naive\"] = sch[\"start_utc\"].dt.tz_convert(LOCAL_TZ).dt.tz_localize(None)\n",
    "\n",
    "            sch = sch[(sch[\"start_local_naive\"] >= start) & (sch[\"start_local_naive\"] <= end)]\n",
    "            for dt in sch[\"start_local_naive\"]:\n",
    "                for h in window_hours(dt, pre_h, post_h):\n",
    "                    hours.add(h)\n",
    "\n",
    "    return hours\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# NHL from official team CSV feed\n",
    "# -----------------------------\n",
    "def load_nhl_hours(min_dt: pd.Timestamp, max_dt: pd.Timestamp, pre_h: int, post_h: int) -> set[pd.Timestamp]:\n",
    "    start = min_dt.normalize() - pd.Timedelta(days=2)\n",
    "    end = max_dt.normalize() + pd.Timedelta(days=2)\n",
    "\n",
    "    hours = set()\n",
    "\n",
    "    for team in NHL_TEAMS:\n",
    "        url = NHL_FEED_TEMPLATE.format(team=team)\n",
    "        path = os.path.join(CACHE_DIR, f\"nhl_{team}.csv\")\n",
    "        try:\n",
    "            download_if_missing(url, path)\n",
    "            sch = pd.read_csv(path)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        needed = {\"START_DATE\", \"START_TIME\", \"LOCATION\"}\n",
    "        if not needed.issubset(set(sch.columns)):\n",
    "            continue\n",
    "\n",
    "        loc = sch[\"LOCATION\"].astype(str)\n",
    "        local_mask = False\n",
    "        for v in NHL_LOCAL_VENUES:\n",
    "            local_mask = local_mask | loc.str.contains(v, case=False, na=False)\n",
    "        sch = sch[local_mask].copy()\n",
    "\n",
    "        sch[\"start_local_naive\"] = pd.to_datetime(\n",
    "            sch[\"START_DATE\"].astype(str).str.strip() + \" \" + sch[\"START_TIME\"].astype(str).str.strip(),\n",
    "            format=\"%m/%d/%Y %I:%M %p\",\n",
    "            errors=\"coerce\",\n",
    "        )\n",
    "        sch = sch.dropna(subset=[\"start_local_naive\"])\n",
    "        sch = sch[(sch[\"start_local_naive\"] >= start) & (sch[\"start_local_naive\"] <= end)]\n",
    "\n",
    "        for dt in sch[\"start_local_naive\"]:\n",
    "            for h in window_hours(dt, pre_h, post_h):\n",
    "                hours.add(h)\n",
    "\n",
    "    return hours\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# MAIN: combine everything + write flags\n",
    "# -----------------------------\n",
    "def add_all_nynj_game_flags_pre_post():\n",
    "    df = pd.read_csv(WEATHER_FILE, dtype=str)\n",
    "    df = build_weather_datetime(df)\n",
    "\n",
    "    min_dt = df[\"datetime\"].min()\n",
    "    max_dt = df[\"datetime\"].max()\n",
    "\n",
    "    report = {}\n",
    "\n",
    "    # NFL\n",
    "    try:\n",
    "        nfl_hours = load_nfl_metlife_hours(min_dt, max_dt)\n",
    "        df[\"is_nfl_game_hour\"] = df[\"datetime\"].isin(nfl_hours)\n",
    "        report[\"NFL\"] = len(nfl_hours)\n",
    "    except Exception as e:\n",
    "        df[\"is_nfl_game_hour\"] = False\n",
    "        report[\"NFL\"] = f\"FAILED ({e})\"\n",
    "\n",
    "    # MLS\n",
    "    try:\n",
    "        mls_hours = set()\n",
    "        mls_hours |= load_mls_hours(RBNY_ICS_URL, RBNY_ICS_PATH, min_dt, max_dt, MLS_PRE, MLS_POST)\n",
    "        mls_hours |= load_mls_hours(NYCFC_ICS_URL, NYCFC_ICS_PATH, min_dt, max_dt, MLS_PRE, MLS_POST)\n",
    "        df[\"is_mls_game_hour\"] = df[\"datetime\"].isin(mls_hours)\n",
    "        report[\"MLS\"] = len(mls_hours)\n",
    "    except Exception as e:\n",
    "        df[\"is_mls_game_hour\"] = False\n",
    "        report[\"MLS\"] = f\"FAILED ({e})\"\n",
    "\n",
    "    # MLB\n",
    "    mlb_hours = set()\n",
    "    yank_ok = download_mlb_csv_from_page(YANKEES_CSV_PAGE, YANKEES_CSV_PATH)\n",
    "    mets_ok = download_mlb_csv_from_page(METS_CSV_PAGE, METS_CSV_PATH)\n",
    "    try:\n",
    "        if yank_ok:\n",
    "            mlb_hours |= load_mlb_hours(YANKEES_CSV_PATH, min_dt, max_dt, MLB_PRE, MLB_POST)\n",
    "        if mets_ok:\n",
    "            mlb_hours |= load_mlb_hours(METS_CSV_PATH, min_dt, max_dt, MLB_PRE, MLB_POST)\n",
    "        df[\"is_mlb_game_hour\"] = df[\"datetime\"].isin(mlb_hours)\n",
    "        report[\"MLB\"] = len(mlb_hours)\n",
    "    except Exception as e:\n",
    "        df[\"is_mlb_game_hour\"] = False\n",
    "        report[\"MLB\"] = f\"FAILED ({e})\"\n",
    "\n",
    "    # NBA\n",
    "    try:\n",
    "        nba_hours = load_nba_hours(min_dt, max_dt, NBA_PRE, NBA_POST)\n",
    "        df[\"is_nba_game_hour\"] = df[\"datetime\"].isin(nba_hours)\n",
    "        report[\"NBA\"] = len(nba_hours)\n",
    "    except Exception as e:\n",
    "        df[\"is_nba_game_hour\"] = False\n",
    "        report[\"NBA\"] = f\"FAILED ({e})\"\n",
    "\n",
    "    # NHL\n",
    "    try:\n",
    "        nhl_hours = load_nhl_hours(min_dt, max_dt, NHL_PRE, NHL_POST)\n",
    "        df[\"is_nhl_game_hour\"] = df[\"datetime\"].isin(nhl_hours)\n",
    "        report[\"NHL\"] = len(nhl_hours)\n",
    "    except Exception as e:\n",
    "        df[\"is_nhl_game_hour\"] = False\n",
    "        report[\"NHL\"] = f\"FAILED ({e})\"\n",
    "\n",
    "    # Any game\n",
    "    df[\"is_any_nynj_game_hour\"] = (\n",
    "        df[\"is_nfl_game_hour\"]\n",
    "        | df[\"is_mls_game_hour\"]\n",
    "        | df[\"is_mlb_game_hour\"]\n",
    "        | df[\"is_nba_game_hour\"]\n",
    "        | df[\"is_nhl_game_hour\"]\n",
    "    )\n",
    "\n",
    "    df.to_csv(WEATHER_FILE, index=False)\n",
    "\n",
    "    print(\"Saved:\", WEATHER_FILE)\n",
    "    print(\"Cache dir:\", CACHE_DIR)\n",
    "    print(\"\\n=== Build report (hour-set sizes) ===\")\n",
    "    for k, v in report.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "    print(\"\\n=== Flag counts in your dataset ===\")\n",
    "    for col in [\"is_nfl_game_hour\",\"is_mls_game_hour\",\"is_mlb_game_hour\",\"is_nba_game_hour\",\"is_nhl_game_hour\",\"is_any_nynj_game_hour\"]:\n",
    "        print(col, \"=\", int(df[col].sum()))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    add_all_nynj_game_flags_pre_post()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
