{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "629246e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: /Users/tenzinchoedhen/Desktop/PABT_Weekly_Fact.csv\n",
      "Rows: 2797\n",
      "   Start_Date   End_Date Month_Year            Carrier  Bus_Volume  \\\n",
      "1  2020-12-07 2020-12-11  12/1/2020            ACADEMY        37.0   \n",
      "2  2020-12-07 2020-12-11  12/1/2020      C&J BUS LINES         1.0   \n",
      "3  2020-12-07 2020-12-11  12/1/2020          COACH USA        83.0   \n",
      "4  2020-12-07 2020-12-11  12/1/2020             DECAMP         0.0   \n",
      "5  2020-12-07 2020-12-11  12/1/2020          GREYHOUND        33.0   \n",
      "6  2020-12-07 2020-12-11  12/1/2020   HCEE - COMMUNITY        74.0   \n",
      "7  2020-12-07 2020-12-11  12/1/2020           LAKELAND        13.0   \n",
      "8  2020-12-07 2020-12-11  12/1/2020              MARTZ        14.0   \n",
      "9  2020-12-07 2020-12-11  12/1/2020         NJ TRANSIT      2199.0   \n",
      "10 2020-12-07 2020-12-11  12/1/2020  PETER PAN_BONANZA        12.0   \n",
      "\n",
      "    Passenger_Volume  \n",
      "1              613.0  \n",
      "2                8.0  \n",
      "3             1605.0  \n",
      "4                0.0  \n",
      "5              702.0  \n",
      "6             1199.0  \n",
      "7              280.0  \n",
      "8              475.0  \n",
      "9            27174.0  \n",
      "10             251.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIG (edit paths + formats)\n",
    "# ----------------------------\n",
    "BUS_PATH = \"/Users/tenzinchoedhen/Desktop/PABT_Bus_Cleaned.csv\"\n",
    "PASS_PATH = \"/Users/tenzinchoedhen/Desktop/PABT_Passenger_Cleaned.csv\"\n",
    "OUTPUT_PATH = \"/Users/tenzinchoedhen/Desktop/PABT_Weekly_Fact.csv\"\n",
    "\n",
    "DATE_COLS = [\"Start_Date\", \"End_Date\"]\n",
    "CARRIER_COL = \"Carrier\"\n",
    "VOLUME_COL = \"Volume\"\n",
    "\n",
    "# ----------------------------\n",
    "# LOAD\n",
    "# ----------------------------\n",
    "bus = pd.read_csv(BUS_PATH)\n",
    "pas = pd.read_csv(PASS_PATH)\n",
    "\n",
    "# ----------------------------\n",
    "# CLEAN + STANDARDIZE\n",
    "# ----------------------------\n",
    "def standardize(df: pd.DataFrame, volume_name: str) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    # normalize column names\n",
    "    df.columns = [c.strip().replace(\" \", \"_\") for c in df.columns]\n",
    "\n",
    "    # parse dates\n",
    "    for c in DATE_COLS:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_datetime(df[c], errors=\"coerce\")\n",
    "\n",
    "    # clean carrier\n",
    "    if CARRIER_COL in df.columns:\n",
    "        df[CARRIER_COL] = (\n",
    "            df[CARRIER_COL]\n",
    "            .astype(str)\n",
    "            .str.strip()\n",
    "            .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "        )\n",
    "\n",
    "    # ensure volume numeric\n",
    "    if VOLUME_COL in df.columns:\n",
    "        df[VOLUME_COL] = (\n",
    "            df[VOLUME_COL]\n",
    "            .astype(str)\n",
    "            .str.replace(\",\", \"\", regex=False)\n",
    "            .str.strip()\n",
    "        )\n",
    "        df[VOLUME_COL] = pd.to_numeric(df[VOLUME_COL], errors=\"coerce\")\n",
    "\n",
    "    # keep required columns\n",
    "    keep = [c for c in DATE_COLS if c in df.columns] + [CARRIER_COL, VOLUME_COL]\n",
    "    df = df[keep]\n",
    "\n",
    "    # rename volume\n",
    "    df = df.rename(columns={VOLUME_COL: volume_name})\n",
    "\n",
    "    # aggregate\n",
    "    group_cols = [c for c in DATE_COLS if c in df.columns] + [CARRIER_COL]\n",
    "    df = df.groupby(group_cols, dropna=False, as_index=False)[volume_name].sum()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "bus_std = standardize(bus, \"Bus_Volume\")\n",
    "pas_std = standardize(pas, \"Passenger_Volume\")\n",
    "\n",
    "# ----------------------------\n",
    "# MERGE\n",
    "# ----------------------------\n",
    "join_cols = [c for c in DATE_COLS if c in bus_std.columns and c in pas_std.columns] + [CARRIER_COL]\n",
    "\n",
    "pabt_weekly = bus_std.merge(\n",
    "    pas_std,\n",
    "    on=join_cols,\n",
    "    how=\"outer\",\n",
    "    validate=\"m:m\"\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# ADD Month_Year in m/d/yyyy format\n",
    "# ----------------------------\n",
    "if \"Start_Date\" in pabt_weekly.columns:\n",
    "    pabt_weekly[\"Month_Year\"] = (\n",
    "        pabt_weekly[\"Start_Date\"]\n",
    "        .dt.to_period(\"M\")\n",
    "        .dt.to_timestamp()\n",
    "        .dt.strftime(\"%-m/%-d/%Y\")   # Linux/Mac\n",
    "    )\n",
    "\n",
    "# ----------------------------\n",
    "# OUTPUT\n",
    "# ----------------------------\n",
    "preferred_order = [\"Start_Date\", \"End_Date\", \"Month_Year\", \"Carrier\", \"Bus_Volume\", \"Passenger_Volume\"]\n",
    "cols = [c for c in preferred_order if c in pabt_weekly.columns] + [c for c in pabt_weekly.columns if c not in preferred_order]\n",
    "\n",
    "pabt_weekly = pabt_weekly[cols].sort_values(\n",
    "    [c for c in [\"Start_Date\", \"Carrier\"] if c in pabt_weekly.columns]\n",
    ")\n",
    "\n",
    "pabt_weekly.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "print(\"✅ Saved:\", OUTPUT_PATH)\n",
    "print(\"Rows:\", len(pabt_weekly))\n",
    "print(pabt_weekly.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11eca26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: /Users/tenzinchoedhen/Desktop/PABT_Monthly_Fact.csv\n",
      "Rows: 648\n",
      "    Month_Year            Carrier  Bus_Volume  Passenger_Volume\n",
      "156  12/1/2020            ACADEMY       105.0            1762.0\n",
      "157  12/1/2020      C&J BUS LINES         4.0              28.0\n",
      "158  12/1/2020          COACH USA       329.0            6000.0\n",
      "159  12/1/2020             DECAMP         0.0               0.0\n",
      "160  12/1/2020          GREYHOUND       143.0            3522.0\n",
      "161  12/1/2020   HCEE - COMMUNITY       218.0            3524.0\n",
      "162  12/1/2020           LAKELAND        50.0            1038.0\n",
      "163  12/1/2020              MARTZ        53.0            1857.0\n",
      "164  12/1/2020         NJ TRANSIT      8364.0          101072.0\n",
      "165  12/1/2020  PETER PAN_BONANZA        40.0            1014.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "INPUT_PATH = \"/Users/tenzinchoedhen/Desktop/PABT_Weekly_Fact.csv\"\n",
    "OUTPUT_PATH = \"/Users/tenzinchoedhen/Desktop/PABT_Monthly_Fact.csv\"\n",
    "\n",
    "df = pd.read_csv(INPUT_PATH)\n",
    "\n",
    "# Ensure Month_Year exists (and normalize if needed)\n",
    "if \"Month_Year\" not in df.columns:\n",
    "    df[\"Start_Date\"] = pd.to_datetime(df[\"Start_Date\"], errors=\"coerce\")\n",
    "    df[\"Month_Year\"] = (\n",
    "        df[\"Start_Date\"]\n",
    "        .dt.to_period(\"M\")\n",
    "        .dt.to_timestamp()\n",
    "        .dt.strftime(\"%-m/%-d/%Y\")   # Mac/Linux\n",
    "    )\n",
    "\n",
    "# Make sure numeric fields are numeric\n",
    "for col in [\"Bus_Volume\", \"Passenger_Volume\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "# Monthly aggregation\n",
    "monthly_fact = (\n",
    "    df.groupby([\"Month_Year\", \"Carrier\"], as_index=False)\n",
    "      .agg({\n",
    "          \"Bus_Volume\": \"sum\",\n",
    "          \"Passenger_Volume\": \"sum\"\n",
    "      })\n",
    ")\n",
    "\n",
    "# Optional: sort nicely (convert Month_Year back to datetime for sorting)\n",
    "monthly_fact[\"_month_sort\"] = pd.to_datetime(monthly_fact[\"Month_Year\"], errors=\"coerce\")\n",
    "monthly_fact = monthly_fact.sort_values([\"_month_sort\", \"Carrier\"]).drop(columns=[\"_month_sort\"])\n",
    "\n",
    "monthly_fact.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "print(\"✅ Saved:\", OUTPUT_PATH)\n",
    "print(\"Rows:\", len(monthly_fact))\n",
    "print(monthly_fact.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70cc0986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traffic rows: 104857\n",
      "Merged rows : 104857\n",
      "Speed match rate: 9.11%\n",
      "Saved: /Users/tenzinchoedhen/Desktop/PABT_Traffic_with_Mobility_Speeds.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIG (edit these paths)\n",
    "# ----------------------------\n",
    "TRAFFIC_PATH = \"/Users/tenzinchoedhen/Desktop/PABT_Traffic_Sampled_Cleaned.csv\"          # or .txt\n",
    "SPEED_PATH   = \"/Users/tenzinchoedhen/Desktop/Mobility_Speeds_Cleaned.csv\"      # or .txt\n",
    "OUTPUT_PATH  = \"/Users/tenzinchoedhen/Desktop/PABT_Traffic_with_Mobility_Speeds.csv\"\n",
    "\n",
    "# If your inputs are .txt, set delimiter (tab is common)\n",
    "TRAFFIC_IS_TXT = False\n",
    "SPEED_IS_TXT   = False\n",
    "TRAFFIC_DELIM  = \"\\t\"\n",
    "SPEED_DELIM    = \"\\t\"\n",
    "\n",
    "# ----------------------------\n",
    "# LOAD\n",
    "# ----------------------------\n",
    "if TRAFFIC_IS_TXT:\n",
    "    traffic_df = pd.read_csv(TRAFFIC_PATH, delimiter=TRAFFIC_DELIM)\n",
    "else:\n",
    "    traffic_df = pd.read_csv(TRAFFIC_PATH)\n",
    "\n",
    "if SPEED_IS_TXT:\n",
    "    speed_df = pd.read_csv(SPEED_PATH, delimiter=SPEED_DELIM)\n",
    "else:\n",
    "    speed_df = pd.read_csv(SPEED_PATH)\n",
    "\n",
    "# ----------------------------\n",
    "# CLEAN / STANDARDIZE COLUMNS\n",
    "# ----------------------------\n",
    "# Strip whitespace from column names\n",
    "traffic_df.columns = traffic_df.columns.str.strip()\n",
    "speed_df.columns = speed_df.columns.str.strip()\n",
    "\n",
    "# ---- Ensure numeric join keys in traffic ----\n",
    "# traffic keys: Fac, Month, YR\n",
    "for col in [\"FAC\", \"Month\", \"Yr\"]:\n",
    "    if col in traffic_df.columns:\n",
    "        traffic_df[col] = pd.to_numeric(traffic_df[col], errors=\"coerce\")\n",
    "\n",
    "# ---- Parse Month_Year in speed ----\n",
    "# speed keys: Facility_Order, Month_Year -> Month, YR\n",
    "if \"Month_Year\" not in speed_df.columns:\n",
    "    raise ValueError(\"Speed table must contain 'Month_Year' column.\")\n",
    "\n",
    "# Try robust parsing for Month_Year (handles '2025-01', 'Jan-2025', '01/2025', etc.)\n",
    "speed_df[\"Month_Year_parsed\"] = pd.to_datetime(speed_df[\"Month_Year\"], errors=\"coerce\")\n",
    "\n",
    "# If parsing failed for many rows, try common formats explicitly\n",
    "if speed_df[\"Month_Year_parsed\"].isna().mean() > 0.2:\n",
    "    # Try MM/YYYY\n",
    "    speed_df[\"Month_Year_parsed\"] = pd.to_datetime(speed_df[\"Month_Year\"], format=\"%m/%Y\", errors=\"coerce\")\n",
    "\n",
    "if speed_df[\"Month_Year_parsed\"].isna().mean() > 0.2:\n",
    "    # Try Mon-YYYY\n",
    "    speed_df[\"Month_Year_parsed\"] = pd.to_datetime(speed_df[\"Month_Year\"], format=\"%b-%Y\", errors=\"coerce\")\n",
    "\n",
    "# Create Month and YR from parsed date\n",
    "speed_df[\"Month\"] = speed_df[\"Month_Year_parsed\"].dt.month\n",
    "speed_df[\"YR\"] = speed_df[\"Month_Year_parsed\"].dt.year\n",
    "\n",
    "# Ensure Facility_Order is numeric (since you said Fac == Facility_Order)\n",
    "if \"Facility_Order\" not in speed_df.columns:\n",
    "    raise ValueError(\"Speed table must contain 'Facility_Order' column.\")\n",
    "\n",
    "speed_df[\"Facility_Order\"] = pd.to_numeric(speed_df[\"Facility_Order\"], errors=\"coerce\")\n",
    "\n",
    "# Drop speed rows where we can't build join keys (optional but recommended)\n",
    "speed_df_clean = speed_df.dropna(subset=[\"Facility_Order\", \"Month\", \"YR\"]).copy()\n",
    "\n",
    "# Optional: If mobility table has duplicates per (Facility_Order, Month, YR),\n",
    "# pick the first (or you can aggregate)\n",
    "dup_mask = speed_df_clean.duplicated(subset=[\"Facility_Order\", \"Month\", \"YR\"], keep=False)\n",
    "if dup_mask.any():\n",
    "    # Keep the first occurrence (safe default)\n",
    "    speed_df_clean = speed_df_clean.sort_values(by=[\"Facility_Order\", \"YR\", \"Month\"]).drop_duplicates(\n",
    "        subset=[\"Facility_Order\", \"Month\", \"YR\"],\n",
    "        keep=\"first\"\n",
    "    )\n",
    "\n",
    "# ----------------------------\n",
    "# LEFT JOIN (keeps ALL traffic records)\n",
    "# ----------------------------\n",
    "merged_df = traffic_df.merge(\n",
    "    speed_df_clean,\n",
    "    left_on=[\"FAC\", \"Month\", \"Yr\"],\n",
    "    right_on=[\"Facility_Order\", \"Month\", \"YR\"],\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_speed\")\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# QUICK VALIDATION\n",
    "# ----------------------------\n",
    "print(\"Traffic rows:\", len(traffic_df))\n",
    "print(\"Merged rows :\", len(merged_df))\n",
    "\n",
    "# How many traffic rows found a matching speed row?\n",
    "# (Avg_Speed might be named differently; adjust if needed.)\n",
    "speed_match_col = \"Avg_Speed\" if \"Avg_Speed\" in merged_df.columns else None\n",
    "if speed_match_col:\n",
    "    match_rate = merged_df[speed_match_col].notna().mean()\n",
    "    print(f\"Speed match rate: {match_rate:.2%}\")\n",
    "else:\n",
    "    # fallback: check if Month_Year_parsed came through\n",
    "    match_rate = merged_df[\"Month_Year_parsed\"].notna().mean() if \"Month_Year_parsed\" in merged_df.columns else None\n",
    "    if match_rate is not None:\n",
    "        print(f\"Speed match rate (by Month_Year_parsed): {match_rate:.2%}\")\n",
    "    else:\n",
    "        print(\"Could not compute match rate (no obvious speed columns found).\")\n",
    "\n",
    "# ----------------------------\n",
    "# SAVE\n",
    "# ----------------------------\n",
    "merged_df.to_csv(OUTPUT_PATH, index=False)\n",
    "print(\"Saved:\", OUTPUT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55f8076c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_z/yxlqx0wn2l7fv2p0ltxqh0r40000gn/T/ipykernel_73440/3183238373.py:11: DtypeWarning: Columns (31,33) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  traffic_df = pd.read_csv(TRAFFIC_PATH)\n",
      "/var/folders/_z/yxlqx0wn2l7fv2p0ltxqh0r40000gn/T/ipykernel_73440/3183238373.py:23: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  traffic_df[\"DATE\"] = pd.to_datetime(traffic_df[\"DATE\"], errors=\"coerce\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traffic rows: 104857\n",
      "Merged rows : 104857\n",
      "LEFT JOIN completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "TRAFFIC_PATH = \"/Users/tenzinchoedhen/Desktop/PABT_Traffic_with_Mobility_Speeds.csv\"\n",
    "WEATHER_PATH = \"/Users/tenzinchoedhen/Desktop/Weather_Events_Games_Data.csv\"\n",
    "OUTPUT_PATH = \"/Users/tenzinchoedhen/Desktop/Traffic_Speed_External_Data.csv.csv\"\n",
    "\n",
    "# -------------------------\n",
    "# Load both CSVs\n",
    "# -------------------------\n",
    "traffic_df = pd.read_csv(TRAFFIC_PATH)\n",
    "weather_df = pd.read_csv(WEATHER_PATH)\n",
    "\n",
    "# -------------------------\n",
    "# Clean column names\n",
    "# -------------------------\n",
    "traffic_df.columns = traffic_df.columns.str.strip()\n",
    "weather_df.columns = weather_df.columns.str.strip()\n",
    "\n",
    "# -------------------------\n",
    "# Standardize DATE format\n",
    "# -------------------------\n",
    "traffic_df[\"DATE\"] = pd.to_datetime(traffic_df[\"DATE\"], errors=\"coerce\")\n",
    "weather_df[\"DATE\"] = pd.to_datetime(weather_df[\"DATE\"], errors=\"coerce\")\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Perform LEFT JOIN\n",
    "# -------------------------\n",
    "merged_df = traffic_df.merge(\n",
    "    weather_df,\n",
    "    on=[\"DATE\", \"TIME\"],\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_weather\")\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Validation\n",
    "# -------------------------\n",
    "print(\"Traffic rows:\", len(traffic_df))\n",
    "print(\"Merged rows :\", len(merged_df))\n",
    "\n",
    "# Save result\n",
    "merged_df.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "print(\"LEFT JOIN completed successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
